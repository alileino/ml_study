{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction\n",
    "The project is split into three files: knn.py, cv.py and agaricus_predict.py. The file knn.py contains the k-Nearest Neighbor algorithm. The file cv.py contains the k-Fold Cross-Validation algorithm, and two scoring measures. The file agaricus_predict.py preprocesses a data set and uses the aforementioned algorithms for prediction. The files are listed in full in the appendix portion of the document. They require Python 3 and numpy to run, and for plotting matplotlib is also required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN implementation\n",
    "The k-Nearest Neighbor algorithm (KNN) was implemented for classification and regression tasks. The implementation uses numpy for numerical calculations. The public interface was partly mimicked scikit-learn (only the signatures, no code). This was done so that the implementation can be compared easily, and it's already intuitive to use.\n",
    "\n",
    "\n",
    "## The prediction method\n",
    "```python\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Returns the KNN prediction for design matrix X.\n",
    "        :param X: the design matrix X\n",
    "        :return: the predicted values for rows of X\n",
    "        '''\n",
    "        dist = self.__get_distance_matrix(X)\n",
    "        \n",
    "        ranks = np.apply_along_axis(KNN.__rank, 1, dist)\n",
    "        # Using np.nan to denote values that are not nearest neighbors\n",
    "        indicators = np.where(ranks < self.n_neighbors, 1, np.nan)\n",
    "\n",
    "        y = np.multiply(indicators, self.y)\n",
    "\n",
    "        # Construct new matrix without nan values\n",
    "        newshape = (np.shape(X)[0], self.n_neighbors)\n",
    "        y = (np.reshape(y[~np.isnan(y)], newshape))\n",
    "\n",
    "        prediction = np.apply_along_axis(self.prediction_func, 1, y)\n",
    "        return prediction\n",
    "```\n",
    "The prediction method takes a design matrix X as parameter. First it computes a 2d-distance matrix between all pairs of rows between the given matrix and the fitted matrix. \n",
    "\n",
    "Then for each row, ranks of the values are calculated. The smallest distance receives the smallest rank (0), next one 1 etc. This is done so that in the future, weighting schemes can take advantage of the ranks. An indicator matrix is constructed which specifies which ranks are in the given neighborhood. The values not in the neighborhood are specified with NaN-values, because they must be differentiated with 0-values. Then a class-matrix is constructed, where the values outside the neighborhood are removed. Finally the prediction function is applied to each row. \n",
    "\n",
    "``` python\n",
    "    def __get_distance_matrix(self, X):\n",
    "        X = X.astype(np.float64)\n",
    "        # Bind self to apply_metric, needed to bind self.X\n",
    "        apply_metric = lambda y : self.metric(y, self.X)\n",
    "        dist = np.apply_along_axis(apply_metric, 1, X)\n",
    "        return dist\n",
    "```\n",
    "Constructing the distance matrix fast was important. The naive implementation of calculating the paired distances in for-loops took minutes to run for the used dataset. The implementation was changed rather easily to much faster (but by no means not optimal) semi-vectorized form. In the used approach, the lambda expression apply_metric calls the metric for a single vector y and the original fitted matrix X. The metric should calculate the paired distances for these, which can usually be done in matrix form. Then this lambda expression is applied to each vector in the prediction matrix X. In the end we end up calculating the distances for all pairs between the fitted X and the given X.\n",
    "\n",
    "For future optimization, at least the regression prediction could be vectorized to matrix form. Also, it may be faster to vectorize most of the apply_along_axis calls, which are not much faster than for-loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Fold cross-validation implementation\n",
    "\n",
    "```python\n",
    "    def split(self, X):\n",
    "        minfoldsize = len(X)//self.n_splits\n",
    "        rem = len(X)-minfoldsize*self.n_splits #remainder, when the splits are not even\n",
    "        foldsizes = np.repeat(minfoldsize, self.n_splits)\n",
    "        foldsizes[:rem] += 1 #remainders are evenly added to first rem folds\n",
    "        endindices = np.cumsum(foldsizes)\n",
    "\n",
    "        for i, (size, endindex) in enumerate(zip(foldsizes, endindices)):\n",
    "            startindex = endindex-size\n",
    "            testindex = np.arange(startindex, endindex)\n",
    "            trainindex = np.concatenate((np.arange(0, startindex), np.arange(endindex, len(X))))\n",
    "            yield trainindex, testindex\n",
    "\n",
    "```\n",
    "The class KFold returns the indices of the folds for the data. It is initalized with the number of splits, and then the function split is used to split a matrix. It only returns the indices, so the actual rows of the split have to be done separately. \n",
    "\n",
    "```python\n",
    "def cv_score(model, X, y, cv, score_func):\n",
    "    '''\n",
    "    Returns the cross-validated\n",
    "    :param model: the model to use\n",
    "    :param X: the design matrix X\n",
    "    :param y: the true values y\n",
    "    :param cv: the cross-validation object to use, which must implement split\n",
    "    :param score_func: the scoring function to use\n",
    "    :return: the mean score of scores returned by score_func for each cross-validation split\n",
    "    '''\n",
    "    scores = list()\n",
    "    for trainindex, testindex in cv.split(X):\n",
    "        trainX = X[trainindex]\n",
    "        trainY = y[trainindex]\n",
    "        testX = X[testindex]\n",
    "        testY = y[testindex]\n",
    "        model.fit(trainX, trainY)\n",
    "        predy = model.predict(testX)\n",
    "        score = score_func(predy, testY)\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)\n",
    "```\n",
    "The function cv_score does the work of calculating the cv-score for any data. It accepts any scoring function which returns a floating point value for prediction and true vector pairs. The function cv_accuracy_score and cv_squares_score are convenience methods which select the scoring method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mushroom data set\n",
    "The data set is the one described in http://archive.ics.uci.edu/ml/datasets/Mushroom . More formally the data set is called \"Agaricus lepiota\".\n",
    "\n",
    "The data set contains attributes about different mushrooms, and a binary target attribute describing whether the mushroom is edible or not. 51,8% of the examples are edible, and 48,2% are not. The baseline accuracy There are 8124 instances and 22 categorical attributes. It was quickly discovered that this prediction task using all the data is very easy: using the entire data set one can acquire 100% accuracy.\n",
    "Attributes were encoded using MultiLabelBinarizer to binary attributes.\n",
    "\n",
    "## Results\n",
    "For debugging purposes, the results were compared to scikit-learn implementation of KNN. Small fluctuations are possible because of tie-breaking situations.\n",
    "For the entire data set, the cross-validated mean prediction accuracy is 100% for n_neighbors=1. Increasing the number of neighbours does not seem to lower the accuracy. By limiting the number of rows used, we can better compare the results. Using 100 rows and n_neighbors=2, the prediction accuracy is 0.95, which is exactly the same as Scikit-Learn's result. \n",
    "\n",
    "The following plots were made by using 200 rows of the original data set using both classification and regression scoring. As said previously, using the entire data set is not very interesting, because accuracy is almost always 100% or very close to it.\n",
    "\n",
    "![](agaricus_neighbors_False.png)\n",
    "![](agaricus_neighbors_True.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendices\n",
    "\n",
    "## knn.py\n",
    "```python\n",
    "import numpy as np\n",
    "from cv import accuracy_score, squared_diff_score\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, n_neighbors=5, metric=None, regression=False):\n",
    "        '''\n",
    "        Initializes the KNN algorithm.\n",
    "        :param n_neighbors: The number of neighbors to use.\n",
    "        :param metric: The metric to use. Any distance measure is accepted, even non-metrics. The function must accept a\n",
    "        1D vector as its first parameter, and a 2D matrix as the second parameter. It should calculate the rowwise distance\n",
    "        between the first vector and all row vectors in the matrix.\n",
    "        :param regression: Set to true for regression calculation. Defaults to False, which is classification.\n",
    "        '''\n",
    "        self.n_neighbors = n_neighbors\n",
    "        if metric == None:\n",
    "            metric = KNN.__euclidean\n",
    "        self.metric = metric\n",
    "        self.regression = regression\n",
    "        self.classification = not regression\n",
    "        if not regression:\n",
    "            self.prediction_func = KNN.__classification_prediction\n",
    "        else:\n",
    "            self.prediction_func = KNN.__regression_prediction\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Metrics typically can't handle ints.\n",
    "        self.X = X.astype(np.float64)\n",
    "        self.y = y\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Returns the KNN prediction for design matrix X.\n",
    "        :param X: the design matrix X\n",
    "        :return: the predicted values for rows of X\n",
    "        '''\n",
    "        dist = self.__get_distance_matrix(X)\n",
    "\n",
    "        ranks = np.apply_along_axis(KNN.__rank, 1, dist)\n",
    "        # Using np.nan to denote values that are not nearest neighbors\n",
    "        indicators = np.where(ranks < self.n_neighbors, 1, np.nan)\n",
    "\n",
    "        y = np.multiply(indicators, self.y)\n",
    "\n",
    "        # Construct new matrix without nan values\n",
    "        newshape = (np.shape(X)[0], self.n_neighbors)\n",
    "        y = (np.reshape(y[~np.isnan(y)], newshape))\n",
    "\n",
    "        prediction = np.apply_along_axis(self.prediction_func, 1, y)\n",
    "        return prediction\n",
    "\n",
    "    def score(self, X, y):\n",
    "        '''\n",
    "        Returns a score for design matrix X and true labels y. Predicted values for X are scored against true values y.\n",
    "        :param X: The design matrix to predict\n",
    "        :param y: The true values of y\n",
    "        :return: If this is a classification task, returns prediction accuracy\n",
    "                 If this is a regression task, returns the sum of the squared errors\n",
    "        '''\n",
    "        predy = self.predict(X)\n",
    "        if self.classification:\n",
    "            return accuracy_score(predy, y)\n",
    "        else:\n",
    "            return squared_diff_score(predy, y)\n",
    "\n",
    "    def __apply_metric_old(self, y):\n",
    "        '''\n",
    "        Applies the metric to each row in self.X paired with y\n",
    "        :param y: the y to give as parameter to metric\n",
    "        :return: 1-dimensional distance vector, where each value is the pairwise distance of corresponding row in X and y\n",
    "        '''\n",
    "        metric = lambda x: self.metric(x, y)\n",
    "        dist = np.apply_along_axis(metric, 1, self.X)\n",
    "        return dist\n",
    "\n",
    "    def __get_distance_matrix(self, X):\n",
    "        X = X.astype(np.float64)\n",
    "        # Bind self to apply_metric, needed to bind self.X\n",
    "        apply_metric = lambda y : self.metric(y, self.X)\n",
    "        dist = np.apply_along_axis(apply_metric, 1, X)\n",
    "        return dist\n",
    "\n",
    "    @staticmethod\n",
    "    def __rank(x):\n",
    "        '''\n",
    "        Returns the ranks of elements of x. Ranks start from 0.\n",
    "        :param x: 1D-vector to rank\n",
    "        :return: The ranks of the elements of x.\n",
    "        '''\n",
    "        order = x.argsort()\n",
    "        ranks = np.empty(len(x), int)\n",
    "        ranks[order] = np.arange(len(x))\n",
    "        return ranks\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def __euclidean(y, X):\n",
    "        '''\n",
    "        The euclidean distance metric\n",
    "        :param y:\n",
    "        :param X:\n",
    "        :return:\n",
    "        '''\n",
    "        axis = np.ndim(X)-1\n",
    "        return np.sqrt(np.sum((y-X)**2, axis=axis))\n",
    "\n",
    "    @staticmethod\n",
    "    def __classification_prediction(y):\n",
    "        values, counts = np.unique(y, return_counts=True)\n",
    "        return values[np.argmax(counts)]\n",
    "\n",
    "    @staticmethod\n",
    "    def __regression_prediction(y):\n",
    "        return np.mean(y)\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cv.py\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class KFold:\n",
    "    def __init__(self, n_splits=4):\n",
    "        self.n_splits = n_splits\n",
    "\n",
    "    def split(self, X):\n",
    "        minfoldsize = len(X)//self.n_splits\n",
    "        rem = len(X)-minfoldsize*self.n_splits #remainder, when the splits are not even\n",
    "        foldsizes = np.repeat(minfoldsize, self.n_splits)\n",
    "        foldsizes[:rem] += 1 #remainders are evenly added to first rem folds\n",
    "        endindices = np.cumsum(foldsizes)\n",
    "\n",
    "        for i, (size, endindex) in enumerate(zip(foldsizes, endindices)):\n",
    "            startindex = endindex-size\n",
    "            testindex = np.arange(startindex, endindex)\n",
    "            trainindex = np.concatenate((np.arange(0, startindex), np.arange(endindex, len(X))))\n",
    "            yield trainindex, testindex\n",
    "\n",
    "def accuracy_score(predy, y):\n",
    "    '''\n",
    "    Returns the accuracy score for predicted y and true y\n",
    "    :param predy: the predicted y\n",
    "    :param y: the true y\n",
    "    :return: the accuracy score\n",
    "    '''\n",
    "    return np.mean(np.where(np.isclose(predy, y), 1, 0))\n",
    "\n",
    "def squared_diff_score(predy, y):\n",
    "    '''\n",
    "    Returns the sum of squared differences between predicted y and true y\n",
    "    :param predy: predicted y\n",
    "    :param y:\n",
    "    :return:\n",
    "    '''\n",
    "    return np.sum((predy - y) ** 2)\n",
    "\n",
    "def cv_score(model, X, y, cv, score_func):\n",
    "    '''\n",
    "    Returns the cross-validated\n",
    "    :param model: the model to use\n",
    "    :param X: the design matrix X\n",
    "    :param y: the true values y\n",
    "    :param cv: the cross-validation object to use, which must implement split\n",
    "    :param score_func: the scoring function to use\n",
    "    :return: the mean score of scores returned by score_func for each cross-validation split\n",
    "    '''\n",
    "    scores = list()\n",
    "    for trainindex, testindex in cv.split(X):\n",
    "        trainX = X[trainindex]\n",
    "        trainY = y[trainindex]\n",
    "        testX = X[testindex]\n",
    "        testY = y[testindex]\n",
    "        model.fit(trainX, trainY)\n",
    "        predy = model.predict(testX)\n",
    "        score = score_func(predy, testY)\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)\n",
    "\n",
    "def cv_accuracy_score(model, X, y, cv=KFold(n_splits=4)):\n",
    "    '''\n",
    "    Returns the cv-score using accuracy scoring.\n",
    "    :see cv_score, accuracy_score\n",
    "    '''\n",
    "    return cv_score(model, X, y, cv=cv, score_func=accuracy_score)\n",
    "\n",
    "def cv_squares_score(model, X, y, cv=KFold(n_splits=4)):\n",
    "    '''\n",
    "    Returns the cv-score using squared difference scoring\n",
    "    :see cv_score, squared_diff_score\n",
    "    '''\n",
    "    return cv_score(model, X, y, cv=cv, score_func=squared_diff_score)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agaricus_predict.py\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from knn import KNN\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from cv import KFold, cv_accuracy_score, cv_squares_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing.label import LabelBinarizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_agaricus_data(row_limit=None):\n",
    "    df = pd.read_csv(\"data/agaricus-lepiota.data\")\n",
    "    # Randomize the rows because they are ordered\n",
    "    df = df.sample(frac=1, random_state=1)\n",
    "    X = df.values[:, 1:]\n",
    "\n",
    "    y = df.values[:, 0]\n",
    "    enc = [MultiLabelBinarizer() for _ in range(len(X[0]))]\n",
    "    newX = np.empty((len(X),0), dtype=np.float64)\n",
    "    for i, encoder in enumerate(enc):\n",
    "        columns = encoder.fit_transform(X[:, i])\n",
    "        newX = np.concatenate((newX, columns), axis=1)\n",
    "    X = newX\n",
    "    y = LabelBinarizer().fit_transform(y)\n",
    "    y = y.flatten()\n",
    "\n",
    "    if row_limit:\n",
    "        X = X[:row_limit, :]\n",
    "        y = y[:row_limit]\n",
    "    return X,y\n",
    "\n",
    "def test_knn(row_limit=None, n_neighbors=5):\n",
    "    X, y = get_agaricus_data(row_limit=row_limit)\n",
    "    models = (KNN(n_neighbors=n_neighbors), KNeighborsClassifier(n_neighbors=n_neighbors))\n",
    "    names = (\"Custom\", \"Scikit-learn\")\n",
    "    scores = []\n",
    "    for model, name in zip(models, names):\n",
    "        score = cv_accuracy_score(model, X, y)\n",
    "        scores.append(score)\n",
    "    diffs = []\n",
    "    for score in scores:\n",
    "        diffs.append(scores[0]-score)\n",
    "    return list(zip(names, scores, diffs))\n",
    "\n",
    "def test_multiple_neighbors(row_limit=None, regression=False, max_neighbors=10):\n",
    "    X,y = get_agaricus_data(row_limit=row_limit)\n",
    "    n_neighbors = np.arange(1,max_neighbors+1)\n",
    "    score_func = cv_accuracy_score\n",
    "    if regression:\n",
    "        score_func = cv_squares_score\n",
    "    scores = []\n",
    "    for k in n_neighbors:\n",
    "        model = KNN(n_neighbors=k, regression=False)\n",
    "\n",
    "        score = score_func(model, X, y)\n",
    "        scores.append(score)\n",
    "    return (n_neighbors, scores)\n",
    "\n",
    "def plot_neighbors(regression=False):\n",
    "    plt.figure()\n",
    "    n_neighbors, scores = test_multiple_neighbors(row_limit=200, regression=regression)\n",
    "    plt.plot(n_neighbors,scores)\n",
    "    ylabel = \"accuracy score\"\n",
    "    type = \"classification\"\n",
    "    if regression:\n",
    "        ylabel = \"squared difference\"\n",
    "        type = \"regression\"\n",
    "    plt.suptitle(\"KNN CV mean vs. neighbors (%s)\" % type)\n",
    "    plt.xlabel(\"neighbors\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.savefig(\"agaricus_neighbors_\" + str(regression) + \".png\")\n",
    "\n",
    "\n",
    "def help_print(scorelist):\n",
    "    print(\"\\n\".join([\"%s: %.5f (diff: %+.5f)\" % (m, s, d) for m, s, d in scorelist]))\n",
    "\n",
    "def plot_everything():\n",
    "    plot_neighbors(regression=True)\n",
    "\n",
    "    plot_neighbors(regression=False)\n",
    "    print(\"For n=100 and n_neighbors=5\")\n",
    "    results = test_knn(row_limit=100, n_neighbors=5)\n",
    "    help_print(results)\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_metadata": {
   "author": "Ali Leino",
   "title": "Implementing KNN and k-Fold CV"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
