{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction\n",
    "The task for this project is to do predictions using the KNN algorithm for the KDD-Cup99 data set. The project uses the implementation of the KNN algorithm developed in the last exercise. Code for those is available at https://github.com/alileino/ml_study. The project uses numpy, pandas and scikit-learn. The KNN implementation was fast enough to produce these plots in less than 30 seconds, so the full data set could be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "The following class handles data loading and preprocessing. Test set contained missing values which were removed without any thought about their meaning, since there were only 16 of them. The training set and test set are combined so that they can be normalized together. Features 1-3 correspond to catecorigal values, which are binary encoded in the encode-function. Columns which only contain one value are removed since they cause divisions by zero later on, and don't provide any value. The objective column was removed before Z-score normalization, and finally the data was split back to the same training and test matrices.\n",
    "```python\n",
    "class KddDataProvider:\n",
    "    '''\n",
    "    Loads and preprocesses KddCup99 data.\n",
    "    '''\n",
    "    label_columns = [1,2,3]\n",
    "    def __init__(self, debug=False):\n",
    "        '''\n",
    "        :param debug: If true, it uses a small part of the entire training data set\n",
    "        '''\n",
    "        trainX, trainY, testX, testY = self.__load_data(debug)\n",
    "        self.trainX = trainX\n",
    "        self.trainY = trainY\n",
    "        self.testX = testX\n",
    "        self.testY = testY\n",
    "\n",
    "    def get_train(self):\n",
    "\n",
    "        return self.trainX, self.trainY\n",
    "\n",
    "    def get_test(self):\n",
    "        return self.testX, self.testY\n",
    "\n",
    "    def __load_data(self, debug):\n",
    "        '''\n",
    "        Loads the training and test data and does preprocessing to them\n",
    "        :param debug:\n",
    "        :return:\n",
    "        '''\n",
    "        dftrain = pd.read_csv(TRAIN_FILE, header=None, names=np.arange(0,42))\n",
    "\n",
    "        if debug:\n",
    "            dftrain = dftrain.sample(frac=0.3, random_state=1)\n",
    "\n",
    "        train_size = len(dftrain)\n",
    "        dftest = pd.read_csv(TEST_FILE, header=None, names=np.arange(0,42))\n",
    "\n",
    "        # Drop rows with NA's\n",
    "        dftest = dftest.dropna(axis=0, how=\"any\")\n",
    "        df = dftrain.append(dftest , ignore_index=True)\n",
    "\n",
    "        for c in df.columns: # Drop columns with only a single value,\n",
    "            if len(df[c].unique()) <= 1:\n",
    "                df = df.drop(c, axis=1)\n",
    "        df = self.__encode(df)\n",
    "\n",
    "        trainY = df.values[:train_size,-1]\n",
    "\n",
    "        testY = df.values[train_size:, -1]\n",
    "        df = df.drop(df.columns[-1], axis=1) # drop the last column\n",
    "\n",
    "        # Z-score normalize the data\n",
    "        df = (df - df.mean()) / (df.max() - df.min())\n",
    "\n",
    "        trainX = df.values[:train_size,:-1]\n",
    "        testX = df.values[train_size:, :-1]\n",
    "        return trainX, trainY, testX, testY\n",
    "\n",
    "    def __encode(self, df):\n",
    "        newdf = df[df.columns[0]]\n",
    "\n",
    "        for label_column in KddDataProvider.label_columns:\n",
    "            newdf = pd.concat((newdf, pd.get_dummies(df[label_column], '', '')\n",
    "            .astype(int)), axis=1, ignore_index=True)\n",
    "\n",
    "        newdf = pd.concat((newdf, df[df.columns[4:]]), axis=1)\n",
    "        newdf.columns = np.arange(0, len(newdf.columns))\n",
    "\n",
    "        return newdf\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting implementation\n",
    "The following code generates a plot of 10-fold cross-validated accuracy against different neighbor values. \n",
    "``` python\n",
    "def plot_model_selection():\n",
    "    neighbors = np.arange(1,11)\n",
    "\n",
    "    data = KddDataProvider()\n",
    "    X, y = data.get_train()\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for k in neighbors:\n",
    "\n",
    "        knn = KNN(n_neighbors=k)\n",
    "        s = cv_accuracy_score(knn, X, y, cv=KFold(n_splits=10))\n",
    "        scores.append(s)\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    y = np.mean(scores, axis=1)\n",
    "\n",
    "    # Uncomment to plot std error bars\n",
    "    # e = np.std(allscores, axis=1)\n",
    "\n",
    "    # plt.errorbar(neighbors+offset, y, yerr=e, lw=2, label=scorer_names[scorer])\n",
    "    plt.plot(neighbors, y, label=\"Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.suptitle(\"10-fold average CV score and Std for KNN\")\n",
    "    plt.xlabel(\"k (neighbors)\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xticks(neighbors)\n",
    "    plt.savefig(path.join(OUTPUT_PATH, \"kdd_model_selection.png\"), format=\"PNG\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix and results implementation\n",
    "The following code plots the confusion matrix and prints the accuracy and f1-score for the test data for 8 neighbors. \n",
    "```python\n",
    "def plot_confusion(truey, predy):\n",
    "    cm = confusion_matrix(truey, predy)\n",
    "    plot_confusion_matrix(cm, np.unique(truey))\n",
    "    plt.savefig(path.join(OUTPUT_PATH, \"kdd_confusion_matrix.png\"), format=\"PNG\")\n",
    "\n",
    "\n",
    "def print_results():\n",
    "    data = KddDataProvider()\n",
    "    trainX, trainY = data.get_train()\n",
    "    testX, testY = data.get_test()\n",
    "    knn = KNN(n_neighbors=8)\n",
    "    knn.fit(trainX, trainY)\n",
    "    predY = knn.predict(testX)\n",
    "    accuracy = accuracy_score(testY, predY)\n",
    "    fscore = f1_score(testY, predY, average=\"weighted\")\n",
    "    print(\"Accuracy:\", accuracy, \"F1-score\", fscore)\n",
    "    plot_confusion(testY, predY)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "For debugging purposes, the results were compared to scikit-learn implementation of KNN. They were identical so not included not in this document. The neighbor value of 1 gave the best results with accuracy of 0.968, but the difference between that and a neighbor value of 8 was less than 0.01, so 8 was chosen as the final hyperparameter value. This was done because 8 is likely to generalize better with the unknown data than 1. \n",
    "\n",
    "\n",
    "![](img/kdd_model_selection.png)\n",
    "![](img/kdd_confusion_matrix.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix shows that classes 1,2 and 3 were predicted very well. However, class 4 had abysmal predictions, with only one correct and 500 incorrect. This contributes most to the lower accuracy values.\n",
    "\n",
    "The final accuracy score for the test set was 0.772, and the f1-score was 0.715."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_metadata": {
   "author": "Ali Leino",
   "title": "Threat-detection in KddCup99 data set"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
